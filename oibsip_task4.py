# -*- coding: utf-8 -*-
"""oibsip-task4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15-otAR1O615QIIx3sixua2O5gJ_eW797

# **LIBRARY IMPORTATION**
"""

import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import PorterStemmer
from sklearn.feature_extraction.text import TfidfVectorizer
import string

"""# **LOADING DATA**"""

dataframe = 'spam.csv'

data = pd.read_csv(dataframe, encoding='latin1')
print(data.head(5))

"""# **DATA DESCRIPTION**"""

print('DATA INFORMATION\n' , data.info())

print('DATA DESCPRITION \n', data.describe())

print('DATA SHAPE \n', data.shape)

"""# **DATA PREPROCESSING**

## **DATA CLEANING**

### **1. Dropping unnecessary columns**
"""

data.drop(data.columns[2:5], axis=1, inplace=True)
print(data.head(5))

"""### **2. Removing stop words**"""

nltk.download('punkt')
nltk.download('stopwords')

def remove_stop_words(text):
    tokens = word_tokenize(text)
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stop_words and token.lower() not in string.punctuation]
    return ' '.join(filtered_tokens)

data['v2'] = data['v2'].apply(remove_stop_words)

print(data.head(3))

"""## **3. Tokenization**"""

def tokenize_text(text):
    return word_tokenize(text)

data['v2'] = data['v2'].apply(tokenize_text)

print(data.head())